import torch
import itertools
import torch.nn as nn
from timm.models.vision_transformer import trunc_normal_
from timm.models.layers import SqueezeExcite
import numpy as np
import itertools
import os
import os.path as ospexit


import torch
import torch.nn as nn 
from torch.autograd import Function
import triton
import triton.language as tl
import math
import torch
import torch.nn as nn
import torch.nn.functional as F
import math



class Conv2d_BN(torch.nn.Sequential):
    def __init__(self, a, b, ks=1, stride=1, pad=0, dilation=1,
                 groups=1, bn_weight_init=1):
        super().__init__()
      
        self.add_module('c', torch.nn.Conv2d(
            a, b, ks, stride, pad, dilation, groups, bias=False))  
        
        
        self.add_module('bn', torch.nn.BatchNorm2d(b))
        
        
        torch.nn.init.constant_(self.bn.weight, bn_weight_init)
        
       
        torch.nn.init.constant_(self.bn.bias, 0)

    @torch.no_grad()  # 禁用梯度计算（推理优化操作）
    def fuse(self):
        
        c, bn = self._modules.values()
        
        
        w = bn.weight / (bn.running_var + bn.eps)**0.5
        
        w = c.weight * w[:, None, None, None]
        
        
        b = bn.bias - bn.running_mean * bn.weight / \
            (bn.running_var + bn.eps)**0.5
        
     
        m = torch.nn.Conv2d(w.size(1) * self.c.groups, w.size(
            0), w.shape[2:], stride=self.c.stride, padding=self.c.padding, dilation=self.c.dilation, groups=self.c.groups,
            device=c.weight.device)  
            
        
        m.weight.data.copy_(w)
        
        
        m.bias.data.copy_(b)
        
        return m  


class BN_Linear(torch.nn.Sequential):
    def __init__(self, a, b, bias=True, std=0.02):
        super().__init__()
        self.add_module('bn', torch.nn.BatchNorm1d(a))
        self.add_module('l', torch.nn.Linear(a, b, bias=bias))
        trunc_normal_(self.l.weight, std=std)
        if bias:
            torch.nn.init.constant_(self.l.bias, 0)

    @torch.no_grad()
    def fuse(self):
        bn, l = self._modules.values()
        w = bn.weight / (bn.running_var + bn.eps)**0.5
        b = bn.bias - self.bn.running_mean * \
            self.bn.weight / (bn.running_var + bn.eps)**0.5
        w = l.weight * w[None, :]
        if l.bias is None:
            b = b @ self.l.weight.T
        else:
            b = (l.weight @ b[:, None]).view(-1) + self.l.bias
        m = torch.nn.Linear(w.size(1), w.size(0), device=l.weight.device)
        m.weight.data.copy_(w)
        m.bias.data.copy_(b)
        return m

class Residual(torch.nn.Module):
    def __init__(self, m, drop=0.):
        super().__init__()
        self.m = m
        self.drop = drop

    def forward(self, x):
        if self.training and self.drop > 0:
            return x + self.m(x) * torch.rand(x.size(0), 1, 1, 1,
                                              device=x.device).ge_(self.drop).div(1 - self.drop).detach()
        else:
            return x + self.m(x)

class FFN(torch.nn.Module):
    def __init__(self, ed, h,):        
        super().__init__()
        self.pw1 = Conv2d_BN(ed, h)
        self.act = torch.nn.ReLU()
        self.pw2 = Conv2d_BN(h, ed, bn_weight_init=0)

    def forward(self, x):
        x = self.pw2(self.act(self.pw1(x)))
        
        return x

class Attention(torch.nn.Module):
    def __init__(self, dim, key_dim, num_heads=8,
                 attn_ratio=4,
                 resolution=14):
        super().__init__()
        self.num_heads = num_heads
        self.scale = key_dim ** -0.5
        self.key_dim = key_dim
        self.nh_kd = nh_kd = key_dim * num_heads
        self.d = int(attn_ratio * key_dim)
        self.dh = int(attn_ratio * key_dim) * num_heads
        self.attn_ratio = attn_ratio
        h = self.dh + nh_kd * 2
        self.qkv = Conv2d_BN(dim, h, ks=1)
        self.proj = torch.nn.Sequential(torch.nn.ReLU(), Conv2d_BN(
            self.dh, dim, bn_weight_init=0))
        self.dw = Conv2d_BN(nh_kd, nh_kd, 3, 1, 1, groups=nh_kd)
        points = list(itertools.product(range(resolution), range(resolution)))
        N = len(points)
        attention_offsets = {}
        idxs = []
        for p1 in points:
            for p2 in points:
                offset = (abs(p1[0] - p2[0]), abs(p1[1] - p2[1]))
                if offset not in attention_offsets:
                    attention_offsets[offset] = len(attention_offsets)
                idxs.append(attention_offsets[offset])
        self.attention_biases = torch.nn.Parameter(
            torch.zeros(num_heads, len(attention_offsets)))
        self.register_buffer('attention_bias_idxs',
                             torch.LongTensor(idxs).view(N, N))

    @torch.no_grad()
    def train(self, mode=True):
        super().train(mode)
        if mode and hasattr(self, 'ab'):
            del self.ab
        else:
            self.ab = self.attention_biases[:, self.attention_bias_idxs]

    def forward(self, x):
        B, _, H, W = x.shape
        N = H * W
        qkv = self.qkv(x)
        q, k, v = qkv.view(B, -1, H, W).split([self.nh_kd, self.nh_kd, self.dh], dim=1)
        q = self.dw(q)
        q, k, v = q.view(B, self.num_heads, -1, N), k.view(B, self.num_heads, -1, N), v.view(B, self.num_heads, -1, N)
        attn = (q.transpose(-2, -1) @ k) * self.scale

        bias = self.attention_biases[:, self.attention_bias_idxs] if self.training else self.ab
        bias = torch.nn.functional.interpolate(bias.unsqueeze(0), size=(attn.size(-2), attn.size(-1)), mode='bicubic')
        attn = attn + bias
        attn = attn.softmax(dim=-1)
        x = (v @ attn.transpose(-2, -1)).reshape(B, -1, H, W)
        x = self.proj(x)
        return x

class RepVGGDW(torch.nn.Module):
    def __init__(self, ed) -> None:
        super().__init__()
        self.conv = Conv2d_BN(ed, ed, 3, 1, 1, groups=ed)
        self.conv1 = Conv2d_BN(ed, ed, 1, 1, 0, groups=ed)
        self.dim = ed
    
    def forward(self, x):
        return self.conv(x) + self.conv1(x) + x
    
    @torch.no_grad()
    def fuse(self):
        conv = self.conv.fuse()
        conv1 = self.conv1.fuse()
        
        conv_w = conv.weight
        conv_b = conv.bias
        conv1_w = conv1.weight
        conv1_b = conv1.bias
        
        conv1_w = torch.nn.functional.pad(conv1_w, [1,1,1,1])

        identity = torch.nn.functional.pad(torch.ones(conv1_w.shape[0], conv1_w.shape[1], 1, 1, device=conv1_w.device), [1,1,1,1])

        final_conv_w = conv_w + conv1_w + identity
        final_conv_b = conv_b + conv1_b

        conv.weight.data.copy_(final_conv_w)
        conv.bias.data.copy_(final_conv_b)
        return conv


class SKA(nn.Module):
    def __init__(self, in_channels, groups=8, kernel_size=3):
        super().__init__()
        self.groups = groups
        self.kernel_size = kernel_size
        self.padding = (kernel_size - 1) // 2 

    def forward(self, x: torch.Tensor, w: torch.Tensor) -> torch.Tensor:
        B, C, H, W = x.shape
        ks = self.kernel_size
        pad = self.padding
        output = torch.zeros_like(x)
        
        for kh in range(ks):
            for kw in range(ks):
            
                hin_start = max(0, kh - pad)
                hin_end = min(H, H + kh - pad)
                win_start = max(0, kw - pad)
                win_end = min(W, W + kw - pad)
                hin = slice(hin_start, hin_end)
                win = slice(win_start, win_end)
                
                
                hout_start = max(0, pad - kh)
                hout_end = min(H, H + pad - kh)
                wout_start = max(0, pad - kw)
                wout_end = min(W, W + pad - kw)
                hout = slice(hout_start, hout_end)
                wout = slice(wout_start, wout_end)
                
                
                x_slice = x[:, :, hin, win]
                w_slice = w[:, :, kh * ks + kw, hout, wout]
                
               
                w_expanded = w_slice.repeat_interleave(C // self.groups, dim=1)
                output[:, :, hout, wout] += x_slice * w_expanded
        
        return output


class LKP(nn.Module):
    def __init__(self, dim, lks, sks, groups):
        super().__init__()
        self.cv1 = Conv2d_BN(dim, dim // 2)
        self.act = nn.ReLU()
        self.cv2 = Conv2d_BN(dim // 2, dim // 2, ks=lks, pad=(lks - 1) // 2, groups=dim // 2)
        self.cv3 = Conv2d_BN(dim // 2, dim // 2)
        self.cv4 = nn.Conv2d(dim // 2, sks ** 2 * groups, kernel_size=1)

        self.norm = nn.GroupNorm(num_groups=groups, num_channels=sks ** 2 * groups)     
        
        self.sks = sks
        self.groups = groups
        self.dim = dim
        

    def forward(self, x):
        x = self.act(self.cv3(self.cv2(self.act(self.cv1(x)))))
        w = self.norm(self.cv4(x))
        b, _, h, w_size = w.size()
        w = w.view(b, self.groups, self.sks ** 2, h, w_size)
        return w    

class LSConv(nn.Module):
    def __init__(self, dim):
        super(LSConv, self).__init__()
        self.lkp = LKP(dim, lks=7, sks=3, groups=8)
        self.ska = SKA(dim)
        self.bn = nn.BatchNorm2d(dim)

    def forward(self, x):
        x_lkp=self.lkp(x)          
        # print(x_lkp.shape)
        return self.bn(self.ska(x, x_lkp)) + x
    


class enconder1(torch.nn.Module):
    def __init__(self,
                 in_channels  ,  
                 out_channels,  
                 ):  
        super().__init__()

        
        self.Stem = torch.nn.Sequential(Conv2d_BN(in_channels, out_channels // 4, 3, 2, 1), torch.nn.ReLU(),
                                Conv2d_BN(out_channels // 4, out_channels // 2, 3, 1, 1), torch.nn.ReLU(),
                                Conv2d_BN(out_channels // 2, out_channels, 3, 1, 1)
                           )
        
        
        
        self.DW = RepVGGDW(out_channels)  
        self.se = SqueezeExcite(out_channels) 
        
         
        
           
        self.LSConv = LSConv(out_channels) 

        
        self.ffn = Residual(
            FFN(out_channels, out_channels))  

    def forward(self, x):
        # print(x.shape)
        
        x= self.Stem(x)
        # print("STEM",x.shape)
        x = self.DW(x)  
        # print("DW",x.shape)
        x = self.se(x)    
        # print("SE",x.shape)   
        x= self.LSConv(x)
        x = self.ffn(x)  
        
        return x


class enconder2_3(torch.nn.Module):
    def __init__(self,
                 in_channels,  
                 out_channels,
                 kd=16,  
                 nh=8,  
                 ar=4,  
                 resolution=14,  
                 ):  
        super().__init__()

        
        self.DW = RepVGGDW(out_channels)  
        self.se = SqueezeExcite(out_channels)   
        self.LSConv = LSConv(out_channels)  

        
        self.ffn = Residual(
            FFN(out_channels, out_channels))  
        

       
        self.downsample = torch.nn.Sequential(Conv2d_BN(in_channels, in_channels, 3, 2, 1 ,groups=in_channels) ,
                                Conv2d_BN(in_channels, out_channels, 1, 1 ,0)  
                           )

    def forward(self, x):
        # print(x.shape)

        x= self.downsample(x)
        # print("STEM",x.shape)
        x = self.DW(x) 
        # print("DW",x.shape)
        x = self.se(x)    
        # print("SE",x.shape)
        x= self.LSConv(x)
        x = self.ffn(x)  
        
        return x

class enconder4(torch.nn.Module):
    def __init__(self,
                 in_channels, 
                 out_channels,
                 kd=16,  
                 nh=8,  
                 ar=4,  
                 resolution=14,  
                 ):  
        super().__init__()


        
        
        self.DW = RepVGGDW(out_channels)  
        self.se = SqueezeExcite(out_channels) 
        self.MHSA = Residual(
                    Attention(out_channels, kd, nh, ar, resolution=resolution)
                )

        
        self.ffn = Residual(
            FFN(out_channels, out_channels))  # 两层MLP，中间层扩展为2倍通道
        

      
        self.downsample = torch.nn.Sequential(Conv2d_BN(in_channels, in_channels, 3, 2, 1 ,groups=in_channels) ,
                                Conv2d_BN(in_channels, out_channels, 1, 1 ,0)  
                           )

    def forward(self, x):
        # print(x.shape)
        
        x= self.downsample(x)
        # print("STEM",x.shape)
        x = self.DW(x)  
        # print("DW",x.shape)
        x = self.se(x)    
        # print("SE",x.shape)
        x= self.MHSA(x)
        # print("MHSA",x.shape)
        x = self.ffn(x)  
        
        return x



class decoder_1_2_3(torch.nn.Module):
    def __init__(self,
                 in_channels,  
                 out_channels,
                 kd=16,  
                 nh=8,  
                 ar=4,  
                 resolution=14,  
                 ):  
        super().__init__()

        
        self.DW = RepVGGDW(out_channels)  
        self.se = SqueezeExcite(out_channels)  
        self.LSConv = LSConv(out_channels)  

        
        self.ffn = Residual(
            FFN(out_channels, out_channels))  
        

        self.up = torch.nn.Sequential(nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True) , 
                                Conv2d_BN(in_channels, out_channels, 1, 1,0) )  
                               

    def forward(self, x):
        # print(x.shape)
       
        x= self.up(x)
        # print("STEM",x.shape)
        x = self.DW(x)  
        # print("DW",x.shape)
        x = self.se(x)    
        # print("SE",x.shape)
        x= self.LSConv(x)
        x = self.ffn(x)   
        
        return x


class decoder4(torch.nn.Module):
    def __init__(self,
                 in_channels,  
                 out_channels,
                 kd=16,  
                 nh=8,  
                 ar=4,  
                 resolution=14,  
                 ):  
        super().__init__()
        
        
        self.DW = RepVGGDW(out_channels)  
        self.se = SqueezeExcite(out_channels) 
        self.MHSA = Residual(
                    Attention(out_channels, kd, nh, ar, resolution=resolution)
                )

        
        self.ffn = Residual(
            FFN(out_channels, out_channels)) 
    

        
        self.up = torch.nn.Sequential(nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True) , 
                                Conv2d_BN(in_channels, out_channels, 1, 1,0) )  
        
        

    def forward(self, x):
        # print(x.shape)
       
        x= self.up(x)
        # print("STEM",x.shape)
        x = self.DW(x)  
        # print("DW",x.shape)
        x = self.se(x)    
        # print("SE",x.shape)
        x= self.MHSA(x)
        # print("MHSA",x.shape)
        x = self.ffn(x)  
        # print("ffn",x.shape)
        
        return x
    




